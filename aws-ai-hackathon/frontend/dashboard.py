"""Streamlit Dashboard - AI-Driven Self-Healing Network Operations System
Multi-Agent Network Autonomy Platform based on AWS Bedrock AgentCore + Strands Agents + MCP
"""
import sys
import os
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '../../')))

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from pathlib import Path
import json
from datetime import datetime, timedelta
import glob


def load_network_metrics():
    """Load network metrics data"""
    csv_file = 'data/before_after_metrics.csv'
    if Path(csv_file).exists():
        df = pd.read_csv(csv_file)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        # Add resource_id as alias for cell_id for backward compatibility
        df['resource_id'] = df['cell_id']
        # Infer domain from cell_id (cell-A01 -> RAN, cell-B17 -> Transport based on data)
        df['domain'] = df['cell_id'].apply(lambda x: 'RAN' if 'A01' in x else 'Transport' if 'B17' in x else 'Unknown')
        return df
    return None


def load_incidents():
    """Load incident data from agentcore_output.json"""
    agentcore_file = Path('data/agentcore_output.json')
    if not agentcore_file.exists():
        return []
    
    with open(agentcore_file, 'r') as f:
        incidents = json.load(f)
    
    # Convert agentcore output to incident format
    formatted_incidents = []
    for inc in incidents:
        # Determine resource_id and cell_id from domain
        domain = inc.get('domain')
        resource_id = 'cell-A01' if domain == 'RAN' else 'transport-link-B17' if domain == 'Transport' else domain
        cell_id = resource_id if domain == 'RAN' else None
        
        # Determine severity from confidence
        confidence = inc.get('confidence', 0)
        if confidence < 0.7:
            severity = 'high'
        elif confidence < 0.85:
            severity = 'medium'
        else:
            severity = 'low'
        
        formatted_incident = {
            'incident_id': inc.get('incident_id'),
            'domain': domain,
            'resource_id': resource_id,
            'cell_id': cell_id,
            'timestamp': inc.get('timestamp'),
            'severity': severity,
            'root_cause': inc.get('root_cause'),
            'recommended_action': inc.get('recommended_action'),
            'confidence': confidence,
            'can_auto_execute': inc.get('can_auto_execute')
        }
        formatted_incidents.append(formatted_incident)
    
    return sorted(formatted_incidents, key=lambda x: x['timestamp'], reverse=True)


def load_analysis_results(incident_id):
    """Load analysis results from agentcore_output.json"""
    agentcore_file = Path('data/agentcore_output.json')
    if not agentcore_file.exists():
        return None
    
    with open(agentcore_file, 'r') as f:
        incidents = json.load(f)
    
    for inc in incidents:
        if inc.get('incident_id') == incident_id:
            return {
                'incident_id': inc.get('incident_id'),
                'analysis': {
                    'root_cause': inc.get('root_cause'),
                    'explanation': inc.get('root_cause'),  # Use root_cause as explanation
                    'confidence': inc.get('confidence'),
                    'recommended_actions': [{
                        'action_type': inc.get('recommended_action'),
                        'target': inc.get('domain'),
                        'parameters': inc.get('expected_impact', {})
                    }]
                }
            }
    return None


def load_optimization_results(incident_id):
    """Load optimization results - generate from agentcore output and metrics"""
    agentcore_file = Path('data/agentcore_output.json')
    metrics_file = Path('data/before_after_metrics.csv')
    
    if not agentcore_file.exists():
        return []
    
    with open(agentcore_file, 'r') as f:
        incidents = json.load(f)
    
    results = []
    for inc in incidents:
        if inc.get('incident_id') == incident_id:
            expected_impact = inc.get('expected_impact', {})
            before_state = {}
            predicted_after = {}
            
            # Try to get actual before/after from CSV metrics
            if metrics_file.exists():
                df_metrics = pd.read_csv(metrics_file)
                cell_id = inc.get('cell_id') or (inc.get('resource_id') if inc.get('resource_id') else None)
                if cell_id:
                    # Get incident and optimized states from CSV
                    incident_data = df_metrics[(df_metrics['cell_id'] == cell_id) & (df_metrics['status'] == 'incident')]
                    optimized_data = df_metrics[(df_metrics['cell_id'] == cell_id) & (df_metrics['status'] == 'optimized')]
                    
                    if not incident_data.empty and not optimized_data.empty:
                        incident_row = incident_data.iloc[0]
                        optimized_row = optimized_data.iloc[0]
                        before_state = {
                            'latency_ms': incident_row.get('latency_ms'),
                            'packet_loss': incident_row.get('packet_loss'),
                            'throughput_mbps': incident_row.get('throughput_mbps'),
                            'utilization_pct': incident_row.get('utilization_pct'),
                            'energy_kwh': incident_row.get('energy_kwh')
                        }
                        predicted_after = {
                            'latency_ms': optimized_row.get('latency_ms'),
                            'packet_loss': optimized_row.get('packet_loss'),
                            'throughput_mbps': optimized_row.get('throughput_mbps'),
                            'utilization_pct': optimized_row.get('utilization_pct'),
                            'energy_kwh': optimized_row.get('energy_kwh')
                        }
            
            # Fallback to expected_impact if CSV data not available
            if not before_state and expected_impact:
                for metric, values in expected_impact.items():
                    if isinstance(values, dict):
                        before_state[metric] = values.get('before')
                        predicted_after[metric] = values.get('after')
            
            result = {
                'action_id': f"ACT_{incident_id}",
                'incident_id': incident_id,
                'action_type': inc.get('recommended_action'),
                'target': inc.get('domain'),
                'parameters': expected_impact,
                'execution_status': 'completed' if inc.get('can_auto_execute') else 'pending',
                'before_state': before_state,
                'predicted_after': predicted_after,
                'kpi_improvements': {}
            }
            results.append(result)
    
    return results


def display_metrics_overview(df):
    """Display network overview"""
    if df is None or df.empty:
        st.warning("No data available")
        return
    
    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Network Availability", "99.7%", "+0.3%")
    col2.metric("Average MTTR", "58 minutes", "-42 minutes")
    col3.metric("Active Alerts", "2", "-5")
    col4.metric("Energy Saving", "12%", "+2%")
    
    st.markdown("---")
    
    # Time series charts
    st.subheader("üìà Network Health Metrics Trend")
    
    # Select metric
    metric_options = ['latency_ms', 'throughput_mbps', 'packet_loss', 'utilization_pct']
    selected_metric = st.selectbox("Select Metric", metric_options)
    
    # Group by resource
    resources = st.multiselect("Select Resources", df['resource_id'].unique(), default=df['resource_id'].unique()[:5])
    
    filtered_df = df[df['resource_id'].isin(resources)]
    
    fig = px.line(filtered_df, x='timestamp', y=selected_metric, color='resource_id',
                  title=f'{selected_metric} Time Series',
                  labels={'timestamp': 'Time', selected_metric: metric_names.get(selected_metric, selected_metric)})
    st.plotly_chart(fig, use_container_width=True)
    
    # Anomaly markers
    anomaly_df = filtered_df[filtered_df['status'] != 'normal']
    if not anomaly_df.empty:
        st.subheader("‚ö†Ô∏è Incident Timeline")
        fig2 = go.Figure()
        for resource in resources:
            resource_anomalies = anomaly_df[anomaly_df['resource_id'] == resource]
            if not resource_anomalies.empty:
                fig2.add_trace(go.Scatter(
                    x=resource_anomalies['timestamp'],
                    y=resource_anomalies[selected_metric],
                    mode='markers',
                    name=resource,
                    marker=dict(size=10, symbol='x')
                ))
        st.plotly_chart(fig2, use_container_width=True)


def display_incident_analysis():
    """Display incident analysis page"""
    st.header("üîç Incident Analysis")
    
    incidents = load_incidents()
    
    if not incidents:
        st.warning("No incidents detected")
        return
    
    # Select incident
    incident_names = [f"{inc['incident_id']} - {inc.get('cell_id') or inc.get('domain')} ({inc['timestamp'][:19]})" for inc in incidents]
    selected_idx = st.selectbox("Select Incident", range(len(incidents)), format_func=lambda x: incident_names[x])
    selected_incident = incidents[selected_idx]
    
    st.markdown("### üìã Incident Details")
    col1, col2, col3 = st.columns(3)
    col1.metric("Incident ID", selected_incident['incident_id'])
    col2.metric("Resource", selected_incident.get('cell_id') or selected_incident.get('domain'))
    col3.metric("Severity", selected_incident['severity'])
    
    st.json(selected_incident)
    
    # Display analysis results
    st.markdown("### üß† AI Root Cause Analysis")
    analysis = load_analysis_results(selected_incident['incident_id'])
    
    if analysis:
        if isinstance(analysis, dict) and 'analysis' in analysis:
            analysis = analysis['analysis']
        
        st.success(f"**Root Cause**: {analysis.get('root_cause', 'N/A')}")
        st.info(f"**Explanation**: {analysis.get('explanation', 'N/A')}")
        st.metric("AI Confidence", f"{analysis.get('confidence', 0) * 100:.1f}%")
        
        # Recommended actions
        st.markdown("### ‚öôÔ∏è Recommended Optimization Actions")
        recommended_actions = analysis.get('recommended_actions', [])
        
        if recommended_actions:
            for i, action in enumerate(recommended_actions, 1):
                with st.expander(f"Action {i}: {action.get('action_type')}"):
                    st.write(f"**Target**: {action.get('target')}")
                    st.write(f"**Parameters**: {action.get('parameters')}")
        else:
            st.warning("No recommended actions available")
    else:
        st.warning("Analysis results not found")


def display_optimization_results():
    """Display optimization results page"""
    st.header("‚öôÔ∏è Automatic Optimization Engine")
    
    incidents = load_incidents()
    
    if not incidents:
        st.warning("No incidents available")
        return
    
    # Select incident
    incident_names = [f"{inc['incident_id']} - {inc.get('cell_id') or inc.get('domain')}" for inc in incidents]
    selected_idx = st.selectbox("Select Incident", range(len(incidents)), 
                                format_func=lambda x: incident_names[x], key='opt_select')
    selected_incident = incidents[selected_idx]
    
    # Load optimization results
    opt_results = load_optimization_results(selected_incident['incident_id'])
    
    if not opt_results:
        st.warning("No optimization plan available for this incident")
        return
    
    for result in opt_results:
        with st.expander(f"Optimization Plan: {result['action_type']}"):
            st.json({
                'action_id': result['action_id'],
                'target': result['target'],
                'parameters': result['parameters'],
                'execution_status': result['execution_status']
            })
            
            # Before/After comparison
            st.markdown("#### üìä Performance Comparison")
            
            if 'before_state' in result and 'predicted_after' in result:
                before = result['before_state']
                after = result['predicted_after']
                
                col1, col2 = st.columns(2)
                with col1:
                    st.subheader("Before Optimization")
                    st.write(f"Latency: {before.get('latency_ms', 'N/A')} ms")
                    st.write(f"Throughput: {before.get('throughput_mbps', 'N/A')} Mbps")
                    st.write(f"Packet Loss: {before.get('packet_loss', 'N/A')}%")
                    st.write(f"Utilization: {before.get('utilization_pct', 'N/A')}%")
                
                with col2:
                    st.subheader("After Optimization (Predicted)")
                    st.write(f"Latency: {after.get('latency_ms', 'N/A')} ms")
                    st.write(f"Throughput: {after.get('throughput_mbps', 'N/A')} Mbps")
                    st.write(f"Packet Loss: {after.get('packet_loss', 'N/A')}%")
                    st.write(f"Utilization: {after.get('utilization_pct', 'N/A')}%")
                
                # KPI improvements
                st.markdown("#### üìà Expected Improvements")
                improvements = result.get('kpi_improvements', {})
                for kpi, value in improvements.items():
                    if 'reduction' in kpi or 'increase' in kpi:
                        change = "‚Üì" if "reduction" in kpi else "‚Üë"
                        st.metric(kpi.replace('_', ' ').title(), f"{change}{value}%")


def display_kpi_dashboard():
    """Display KPI Dashboard"""
    st.header("üìä Performance Metrics Comparison")
    
    # Simulated KPI data
    kpi_data = {
        'metrics': ['MTTR (minutes)', 'Availability (%)', 'Energy (%)'],
        'before': [120, 99.4, 100],
        'after': [60, 99.7, 88]
    }
    
    df_kpi = pd.DataFrame(kpi_data)
    
    fig = go.Figure(data=[
        go.Bar(name='Before Optimization', x=df_kpi['metrics'], y=df_kpi['before'], marker_color='red'),
        go.Bar(name='After Optimization', x=df_kpi['metrics'], y=df_kpi['after'], marker_color='green')
    ])
    
    fig.update_layout(
        title='KPI Improvement Comparison',
        xaxis_title='Metric',
        yaxis_title='Value',
        barmode='group'
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Improvement percentages
    improvements = [
        ('MTTR', '58 minutes ‚Üí 36 minutes', '-42 minutes'),
        ('Network Availability', '99.4% ‚Üí 99.7%', '+0.3%'),
        ('Energy Consumption', 'Baseline ‚Üí 88%', '-12%')
    ]
    
    col1, col2, col3 = st.columns(3)
    metrics_cols = [col1, col2, col3]
    
    for i, (metric, value, change) in enumerate(improvements):
        with metrics_cols[i]:
            st.metric(metric, value, change)


# Metric name mapping
metric_names = {
    'latency_ms': 'Latency (ms)',
    'throughput_mbps': 'Throughput (Mbps)',
    'packet_loss': 'Packet Loss (%)',
    'utilization_pct': 'Utilization (%)'
}


def display_autonomous_healing_overview():
    """Display self-healing network overview - core value showcase"""
    st.header("ü§ñ Network Self-Healing System Overview")
    st.markdown("""
    <div style='background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); padding: 20px; border-radius: 10px; color: white; margin-bottom: 20px;'>
        <h3 style='color: white; margin: 0;'>Network Autonomic Maintenance Agent</h3>
        <p style='color: rgba(255,255,255,0.9); margin: 10px 0 0 0;'>
            Multi-agent system based on <strong>Amazon Bedrock AgentCore + Strands Agents + MCP</strong><br/>
            Automated closed-loop from alert to recovery: Detect ‚Üí Reason ‚Üí Execute ‚Üí Verify ‚Üí Learn
        </p>
    </div>
    """, unsafe_allow_html=True)
    
    # Core KPI cards
    st.subheader("üìä Core Performance Metrics")
    col1, col2, col3, col4 = st.columns(4)
    
    # Calculate actual MTTR
    incidents = load_incidents()
    optimizations = []
    if incidents:
        for inc in incidents[:5]:  # Last 5 incidents
            opts = load_optimization_results(inc['incident_id'])
            optimizations.extend(opts)
    
    # Simulated MTTR calculation (should calculate from timestamps)
    avg_mttr_min = 4.8 if optimizations else 58.0
    latency_improvement = 170 if optimizations else 0
    energy_saving = 20 if optimizations else 12
    
    col1.metric(
        "Average MTTR", 
        f"{avg_mttr_min:.1f} minutes",
        f"-{58 - avg_mttr_min:.1f} minutes",
        help="Time from alert to recovery (manual operations require 30+ minutes)"
    )
    col2.metric(
        "Latency Improvement", 
        f"{latency_improvement} ms ‚Üì",
        f"{latency_improvement} ms",
        help="Latency reduction after incident recovery"
    )
    col3.metric(
        "Energy Saving", 
        f"{energy_saving}%",
        f"+{energy_saving - 12}%",
        help="Energy consumption reduction after optimization, directly translates to OPEX savings"
    )
    col4.metric(
        "Network Availability", 
        "99.7%",
        "+0.3%",
        help="Self-healing system improves overall availability"
    )
    
    st.markdown("---")


def display_scenario_comparison():
    """Display RAN and Transport scenario comparison"""
    st.header("üåê Cross-Domain Scenario Comparison: RAN + Transport")
    st.markdown("""
    **Core Value**: The same Agent framework uniformly handles wireless (RAN) and transport (Transport) failures,
    demonstrating cross-domain extensibility, extendable to Core, NTN and more domains.
    """)
    
    # Scenario selection
    scenario = st.radio(
        "Select Scenario",
        ["RAN Incident Scenario (cell-A01)", "Transport Incident Scenario (cell-B17)"],
        horizontal=True
    )
    
    if "RAN" in scenario:
        display_ran_scenario()
    else:
        display_transport_scenario()


def display_ran_scenario():
    """RAN scenario: cell overload"""
    st.subheader("üì° Scenario A: RAN Cell Overload (cell-A01)")
    
    # Load data from CSV
    df = load_network_metrics()
    if df is not None:
        ran_df = df[df['cell_id'] == 'cell-A01']
        if not ran_df.empty:
            # Get data for different stages
            normal_data = ran_df[ran_df['status'] == 'normal'].iloc[0] if len(ran_df[ran_df['status'] == 'normal']) > 0 else None
            incident_data = ran_df[ran_df['status'] == 'incident'].iloc[0] if len(ran_df[ran_df['status'] == 'incident']) > 0 else None
            optimized_data = ran_df[ran_df['status'] == 'optimized'].iloc[0] if len(ran_df[ran_df['status'] == 'optimized']) > 0 else None
            
            if normal_data is not None and incident_data is not None and optimized_data is not None:
                stages_data = {
                    'normal': {
                        'latency_ms': normal_data['latency_ms'],
                        'packet_loss': normal_data['packet_loss'],
                        'utilization_pct': normal_data['utilization_pct'],
                        'energy_kwh': normal_data['energy_kwh'],
                        'status': 'normal',
                        'throughput_mbps': normal_data['throughput_mbps']
                    },
                    'incident': {
                        'latency_ms': incident_data['latency_ms'],
                        'packet_loss': incident_data['packet_loss'],
                        'utilization_pct': incident_data['utilization_pct'],
                        'energy_kwh': incident_data['energy_kwh'],
                        'status': 'incident',
                        'throughput_mbps': incident_data['throughput_mbps']
                    },
                    'optimized': {
                        'latency_ms': optimized_data['latency_ms'],
                        'packet_loss': optimized_data['packet_loss'],
                        'utilization_pct': optimized_data['utilization_pct'],
                        'energy_kwh': optimized_data['energy_kwh'],
                        'status': 'optimized',
                        'throughput_mbps': optimized_data['throughput_mbps']
                    }
                }
            else:
                # Fallback to hardcoded data from CSV
                stages_data = {
                    'normal': {'latency_ms': 48, 'packet_loss': 0.1, 'utilization_pct': 42, 'energy_kwh': 2.3, 'status': 'normal', 'throughput_mbps': 120},
                    'incident': {'latency_ms': 310, 'packet_loss': 5.8, 'utilization_pct': 91, 'energy_kwh': 3.9, 'status': 'incident', 'throughput_mbps': 118},
                    'optimized': {'latency_ms': 140, 'packet_loss': 2.2, 'utilization_pct': 69, 'energy_kwh': 3.1, 'status': 'optimized', 'throughput_mbps': 125}
                }
        else:
            # Fallback to hardcoded data from CSV
            stages_data = {
                'normal': {'latency_ms': 48, 'packet_loss': 0.1, 'utilization_pct': 42, 'energy_kwh': 2.3, 'status': 'normal', 'throughput_mbps': 120},
                'incident': {'latency_ms': 310, 'packet_loss': 5.8, 'utilization_pct': 91, 'energy_kwh': 3.9, 'status': 'incident', 'throughput_mbps': 118},
                'optimized': {'latency_ms': 140, 'packet_loss': 2.2, 'utilization_pct': 69, 'energy_kwh': 3.1, 'status': 'optimized', 'throughput_mbps': 125}
            }
    else:
        # Fallback to hardcoded data from CSV
        stages_data = {
            'normal': {'latency_ms': 48, 'packet_loss': 0.1, 'utilization_pct': 42, 'energy_kwh': 2.3, 'status': 'normal', 'throughput_mbps': 120},
            'incident': {'latency_ms': 310, 'packet_loss': 5.8, 'utilization_pct': 91, 'energy_kwh': 3.9, 'status': 'incident', 'throughput_mbps': 118},
            'optimized': {'latency_ms': 140, 'packet_loss': 2.2, 'utilization_pct': 69, 'energy_kwh': 3.1, 'status': 'optimized', 'throughput_mbps': 125}
        }
    
    # Load agent decision from agentcore_output
    incidents = load_incidents()
    agent_decision = None
    for inc in incidents:
        if inc.get('domain') == 'RAN' and 'A01' in inc.get('incident_id', ''):
            agent_decision = {
                'root_cause': inc.get('root_cause', 'RAN congestion, cell-A01 overloaded'),
                'recommended_action': inc.get('recommended_action', 'traffic_steering (load balancing)'),
                'confidence': inc.get('confidence', 0.81),
                'can_auto_execute': inc.get('can_auto_execute', True),
                'expected_impact': {},
                'mttr': '4.5 minutes'
            }
            break
    
    if agent_decision is None:
        agent_decision = {
            'root_cause': 'RAN congestion, cell-A01 overloaded',
            'recommended_action': 'traffic_steering (load balancing)',
            'confidence': 0.81,
            'can_auto_execute': True,
            'expected_impact': {
                'latency': '310‚Üí140 ms',
                'packet_loss': '5.8%‚Üí2.2%',
                'energy': '3.9‚Üí3.1 kWh'
            },
            'mttr': '4.5 minutes'
        }
    
    # Display three-stage comparison
    display_three_stage_comparison(stages_data, agent_decision, "RAN")


def display_transport_scenario():
    """Transport scenario: link congestion"""
    st.subheader("üîó Scenario B: Transport Link Congestion (cell-B17)")
    
    # Load data from CSV
    df = load_network_metrics()
    if df is not None:
        transport_df = df[df['cell_id'] == 'cell-B17']
        if not transport_df.empty:
            # Get data for different stages
            normal_data = transport_df[transport_df['status'] == 'normal'].iloc[0] if len(transport_df[transport_df['status'] == 'normal']) > 0 else None
            incident_data = transport_df[transport_df['status'] == 'incident'].iloc[0] if len(transport_df[transport_df['status'] == 'incident']) > 0 else None
            optimized_data = transport_df[transport_df['status'] == 'optimized'].iloc[0] if len(transport_df[transport_df['status'] == 'optimized']) > 0 else None
            
            if normal_data is not None and incident_data is not None and optimized_data is not None:
                stages_data = {
                    'normal': {
                        'latency_ms': normal_data['latency_ms'],
                        'packet_loss': normal_data['packet_loss'],
                        'utilization_pct': normal_data['utilization_pct'],
                        'energy_kwh': normal_data['energy_kwh'],
                        'status': 'normal',
                        'throughput_mbps': normal_data['throughput_mbps']
                    },
                    'incident': {
                        'latency_ms': incident_data['latency_ms'],
                        'packet_loss': incident_data['packet_loss'],
                        'utilization_pct': incident_data['utilization_pct'],
                        'energy_kwh': incident_data['energy_kwh'],
                        'status': 'incident',
                        'throughput_mbps': incident_data['throughput_mbps']
                    },
                    'optimized': {
                        'latency_ms': optimized_data['latency_ms'],
                        'packet_loss': optimized_data['packet_loss'],
                        'utilization_pct': optimized_data['utilization_pct'],
                        'energy_kwh': optimized_data['energy_kwh'],
                        'status': 'optimized',
                        'throughput_mbps': optimized_data['throughput_mbps']
                    }
                }
            else:
                # Fallback to hardcoded data from CSV
                stages_data = {
                    'normal': {'latency_ms': 55, 'packet_loss': 0.2, 'utilization_pct': 50, 'energy_kwh': 2.9, 'status': 'normal', 'throughput_mbps': 210},
                    'incident': {'latency_ms': 270, 'packet_loss': 4.9, 'utilization_pct': 88, 'energy_kwh': 4.4, 'status': 'incident', 'throughput_mbps': 205},
                    'optimized': {'latency_ms': 115, 'packet_loss': 1.9, 'utilization_pct': 63, 'energy_kwh': 3.5, 'status': 'optimized', 'throughput_mbps': 215}
                }
        else:
            # Fallback to hardcoded data from CSV
            stages_data = {
                'normal': {'latency_ms': 55, 'packet_loss': 0.2, 'utilization_pct': 50, 'energy_kwh': 2.9, 'status': 'normal', 'throughput_mbps': 210},
                'incident': {'latency_ms': 270, 'packet_loss': 4.9, 'utilization_pct': 88, 'energy_kwh': 4.4, 'status': 'incident', 'throughput_mbps': 205},
                'optimized': {'latency_ms': 115, 'packet_loss': 1.9, 'utilization_pct': 63, 'energy_kwh': 3.5, 'status': 'optimized', 'throughput_mbps': 215}
            }
    else:
        # Fallback to hardcoded data from CSV
        stages_data = {
            'normal': {'latency_ms': 55, 'packet_loss': 0.2, 'utilization_pct': 50, 'energy_kwh': 2.9, 'status': 'normal', 'throughput_mbps': 210},
            'incident': {'latency_ms': 270, 'packet_loss': 4.9, 'utilization_pct': 88, 'energy_kwh': 4.4, 'status': 'incident', 'throughput_mbps': 205},
            'optimized': {'latency_ms': 115, 'packet_loss': 1.9, 'utilization_pct': 63, 'energy_kwh': 3.5, 'status': 'optimized', 'throughput_mbps': 215}
        }
    
    # Load agent decision from agentcore_output
    incidents = load_incidents()
    agent_decision = None
    for inc in incidents:
        if inc.get('domain') == 'Transport' and 'B17' in inc.get('incident_id', ''):
            agent_decision = {
                'root_cause': inc.get('root_cause', 'backhaul/transport congestion on link to cell-B17'),
                'recommended_action': inc.get('recommended_action', 'reroute_traffic (traffic rerouting)'),
                'confidence': inc.get('confidence', 0.82),
                'can_auto_execute': inc.get('can_auto_execute', True),
                'expected_impact': {},
                'mttr': '5.2 minutes'
            }
            break
    
    if agent_decision is None:
        agent_decision = {
            'root_cause': 'backhaul/transport congestion on link to cell-B17',
            'recommended_action': 'reroute_traffic (traffic rerouting)',
            'confidence': 0.82,
            'can_auto_execute': True,
            'expected_impact': {
                'latency': '270‚Üí115 ms',
                'packet_loss': '4.9%‚Üí1.9%',
                'energy': '4.4‚Üí3.5 kWh'
            },
            'mttr': '5.2 minutes'
        }
    
    # Display three-stage comparison
    display_three_stage_comparison(stages_data, agent_decision, "Transport")


def display_three_stage_comparison(stages_data, agent_decision, domain):
    """Display comparison of three stages (normal ‚Üí incident ‚Üí optimized)"""
    
    # Agent decision display
    st.markdown("### üß† Agent Decision Process")
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.info(f"**Root Cause**: {agent_decision['root_cause']}")
    with col2:
        confidence_val = agent_decision['confidence']
        if confidence_val > 0.8:
            confidence_color = "üü¢"
        elif confidence_val > 0.7:
            confidence_color = "üü°"
        else:
            confidence_color = "üî¥"
        st.metric("AI Confidence", f"{confidence_color} {confidence_val*100:.0f}%")
    with col3:
        auto_exec = "‚úÖ Auto Execute" if agent_decision['can_auto_execute'] else "‚ö†Ô∏è Requires Manual Approval"
        st.markdown(f"**Execution Mode**: {auto_exec}")
    
    st.markdown(f"**Recommended Action**: `{agent_decision['recommended_action']}`")
    st.markdown(f"**Recovery Time (MTTR)**: {agent_decision['mttr']}")
    
    st.markdown("---")
    
    # Three-stage comparison chart
    st.markdown("### üìà Three-Stage Metrics Comparison")
    
    # Prepare data
    metrics = ['latency_ms', 'packet_loss', 'utilization_pct', 'energy_kwh', 'throughput_mbps']
    metric_labels = {
        'latency_ms': 'Latency (ms)',
        'packet_loss': 'Packet Loss (%)',
        'utilization_pct': 'Utilization (%)',
        'energy_kwh': 'Energy (kWh)',
        'throughput_mbps': 'Throughput (Mbps)'
    }
    
    # Create comparison chart
    fig = go.Figure()
    
    stages = ['normal', 'incident', 'optimized']
    colors = ['#2ecc71', '#e74c3c', '#3498db']
    stage_labels = ['Normal State', 'Incident State', 'Optimized State']
    
    for i, (stage, color, label) in enumerate(zip(stages, colors, stage_labels)):
        values = [stages_data[stage][m] for m in metrics]
        fig.add_trace(go.Bar(
            name=label,
            x=[metric_labels[m] for m in metrics],
            y=values,
            marker_color=color,
            text=[f"{v:.1f}" for v in values],
            textposition='auto'
        ))
    
    fig.update_layout(
        title=f'{domain} Scenario - Three-Stage Metrics Comparison',
        xaxis_title='Metric',
        yaxis_title='Value',
        barmode='group',
        height=400,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1)
    )
    
    st.plotly_chart(fig, use_container_width=True)
    
    # Detailed comparison table
    st.markdown("### üìä Detailed Metrics Comparison")
    
    comparison_df = pd.DataFrame({
        'Metric': [metric_labels[m] for m in metrics],
        'Normal State': [stages_data['normal'][m] for m in metrics],
        'Incident State': [stages_data['incident'][m] for m in metrics],
        'Optimized State': [stages_data['optimized'][m] for m in metrics],
        'Improvement': [
            f"{(stages_data['incident'][m] - stages_data['optimized'][m]) / stages_data['incident'][m] * 100:.1f}%"
            for m in metrics
        ]
    })
    
    st.dataframe(comparison_df, use_container_width=True, hide_index=True)
    
    # Improvement summary
    st.markdown("### ‚úÖ Optimization Results")
    col1, col2, col3, col4 = st.columns(4)
    
    latency_improve = stages_data['incident']['latency_ms'] - stages_data['optimized']['latency_ms']
    loss_improve = stages_data['incident']['packet_loss'] - stages_data['optimized']['packet_loss']
    energy_improve = (stages_data['incident']['energy_kwh'] - stages_data['optimized']['energy_kwh']) / stages_data['incident']['energy_kwh'] * 100
    
    col1.metric("Latency Reduction", f"{latency_improve:.0f} ms", f"{latency_improve} ms")
    col2.metric("Packet Loss Reduction", f"{loss_improve:.1f}%", f"-{loss_improve:.1f}%")
    col3.metric("Energy Reduction", f"{energy_improve:.1f}%", f"-{energy_improve:.1f}%")
    col4.metric("MTTR", agent_decision['mttr'], "Automated")
    
    # Timeline chart
    st.markdown("### ‚è±Ô∏è Incident Recovery Timeline")
    
    timeline_data = {
        'timestamp': [
            'T-0 (Normal)', 'T+0 (Alert)', 'T+2min (Analysis)', 'T+4.5min (Execute)', 'T+5min (Verify)'
        ],
        'latency_ms': [
            stages_data['normal']['latency_ms'],
            stages_data['incident']['latency_ms'],
            stages_data['incident']['latency_ms'],
            stages_data['optimized']['latency_ms'],
            stages_data['optimized']['latency_ms']
        ],
        'packet_loss': [
            stages_data['normal']['packet_loss'],
            stages_data['incident']['packet_loss'],
            stages_data['incident']['packet_loss'],
            stages_data['optimized']['packet_loss'],
            stages_data['optimized']['packet_loss']
        ]
    }
    
    fig_timeline = go.Figure()
    
    fig_timeline.add_trace(go.Scatter(
        x=timeline_data['timestamp'],
        y=timeline_data['latency_ms'],
        mode='lines+markers',
        name='Latency (ms)',
        line={'color': '#3498db', 'width': 2},
        marker={'size': 10}
    ))
    
    # Scale packet loss for display
    scaled_packet_loss = [x * 50 for x in timeline_data['packet_loss']]
    fig_timeline.add_trace(go.Scatter(
        x=timeline_data['timestamp'],
        y=scaled_packet_loss,
        mode='lines+markers',
        name='Packet Loss (√ó50)',
        line={'color': '#e74c3c', 'width': 2},
        marker={'size': 10},
        yaxis='y2'
    ))
    
    fig_timeline.update_layout(
        title='Incident Recovery Timeline',
        xaxis_title='Time Point',
        yaxis_title='Latency (ms)',
        yaxis2=dict(title='Packet Loss (%)', overlaying='y', side='right'),
        height=300,
        hovermode='x unified'
    )
    
    st.plotly_chart(fig_timeline, use_container_width=True)


def display_agent_reasoning_layer():
    """Display detailed information about Agent reasoning layer"""
    st.header("üß† Agent Reasoning Layer (Bedrock AgentCore + Strands Agents)")
    
    st.markdown("""
    **Architecture**: Agents use MCP (Model Context Protocol) to call network tools, 
    implementing the closed-loop: Detect ‚Üí Reason ‚Üí Plan ‚Üí Execute.
    """)
    
    # Select incident to view Agent decisions
    incidents = load_incidents()
    
    if not incidents:
        st.warning("No incident data available")
        return
    
    incident_names = [f"{inc['incident_id']} - {inc.get('cell_id') or inc.get('domain')} ({inc['timestamp'][:19]})" for inc in incidents]
    selected_idx = st.selectbox("Select Incident to View Agent Decision", range(len(incidents)), format_func=lambda x: incident_names[x])
    selected_incident = incidents[selected_idx]
    
    # Load analysis results
    analysis = load_analysis_results(selected_incident['incident_id'])
    optimization_results = load_optimization_results(selected_incident['incident_id'])
    
    if analysis:
        if isinstance(analysis, dict) and 'analysis' in analysis:
            analysis_data = analysis['analysis']
        else:
            analysis_data = analysis
        
        # Agent output display
        st.markdown("### üìã Agent Output (agentcore_output.json)")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üîç Root Cause Analysis")
            st.success(f"**Root Cause**: {analysis_data.get('root_cause', 'N/A')}")
            st.info(f"**Explanation**: {analysis_data.get('explanation', 'N/A')}")
            st.metric("Confidence", f"{analysis_data.get('confidence', 0) * 100:.1f}%")
        
        with col2:
            st.markdown("#### ‚öôÔ∏è Recommended Actions")
            actions = analysis_data.get('recommended_actions', [])
            if actions:
                for i, action in enumerate(actions, 1):
                    st.code(f"Action{i}: {action.get('action_type')}\nTarget: {action.get('target')}\nParameters: {action.get('parameters')}", language=None)
            else:
                st.warning("No recommended actions available")
        
        # If there are associated optimization results, show execution status
        if optimization_results:
            st.markdown("### ‚úÖ Execution Status")
            for opt in optimization_results:
                with st.expander(f"Execution Plan: {opt['action_type']} - {opt['execution_status']}"):
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        st.markdown("**Before Execution State**")
                        before = opt.get('before_state', {})
                        st.json(before)
                    
                    with col2:
                        st.markdown("**Predicted Result**")
                        after = opt.get('predicted_after', {})
                        st.json(after)
                    
                    # KPI improvements
                    improvements = opt.get('kpi_improvements', {})
                    if improvements:
                        st.markdown("**KPI Improvement Metrics**")
                        improvements_cols = st.columns(min(len(improvements), 4))
                        for idx, (kpi, value) in enumerate(list(improvements.items())[:4]):
                            with improvements_cols[idx % 4]:
                                change_icon = "‚Üì" if "reduction" in kpi else "‚Üë"
                                st.metric(
                                    kpi.replace('_', ' ').title()[:20],
                                    f"{change_icon}{value}",
                                    help=kpi
                                )


def display_safety_audit():
    """Display safety audit and manual intervention"""
    st.header("üîí Safety Gate & Audit Log")
    
    st.markdown("""
    **Security Mechanism**: All automated actions are checked by `can_auto_execute`. 
    Low-confidence actions require manual approval.
    All operations are logged to DynamoDB and CloudTrail to ensure traceability and rollback capability.
    """)
    
    # Load optimization results from agentcore_output
    incidents = load_incidents()
    
    if not incidents:
        st.warning("No execution records available")
        return
    
    # Display recent execution records
    executions = []
    for inc in incidents:
        opt_results = load_optimization_results(inc['incident_id'])
        for opt in opt_results:
            executions.append({
                'action_id': opt.get('action_id'),
                'incident_id': inc['incident_id'],
                'action_type': opt.get('action_type'),
                'timestamp': inc.get('timestamp'),
                'execution_status': opt.get('execution_status'),
                'dry_run': not inc.get('can_auto_execute', True),
                'confidence': inc.get('confidence', 0.85)
            })
    
    # Audit log table
    st.markdown("### üìù Execution Audit Log")
    
    if executions:
        audit_df = pd.DataFrame(executions)
        
        # Format timestamp
        audit_df['timestamp'] = pd.to_datetime(audit_df['timestamp']).dt.strftime('%Y-%m-%d %H:%M:%S')
        
        st.dataframe(
            audit_df[['timestamp', 'action_id', 'action_type', 'execution_status', 'dry_run']],
            use_container_width=True,
            hide_index=True
        )
        
        # Statistics
        st.markdown("### üìä Execution Statistics")
        col1, col2, col3, col4 = st.columns(4)
        
        total = len(audit_df)
        auto_exec = len(audit_df[audit_df['dry_run'] == False])
        dry_runs = len(audit_df[audit_df['dry_run'] == True])
        
        col1.metric("Total Executions", total)
        col2.metric("Auto Execute", auto_exec, f"{auto_exec/total*100:.0f}%")
        col3.metric("Dry Run", dry_runs, f"{dry_runs/total*100:.0f}%")
        col4.metric("Success Rate", "100%", "No failures")
    else:
        st.warning("No execution records found")
    
    # Manual intervention description
    st.markdown("---")
    st.markdown("### üë§ Manual Intervention Mechanism")
    
    st.info("""
    **Security Policy**:
    - Confidence < 0.7 ‚Üí Requires manual approval
    - High-risk operations (e.g., power adjustment >5dB) ‚Üí Requires manual approval  
    - All operations have rollback plans
    - No improvement within 5 minutes ‚Üí Automatic rollback
    """)


def display_knowledge_base():
    """Display knowledge retention and learning capability"""
    st.header("üìö Knowledge Base & Experience Learning")
    
    st.markdown("""
    **Knowledge Retention**: The system saves `incident ‚Üí root_cause ‚Üí action ‚Üí outcome` as an experience database.
    When encountering similar failures next time, the Agent can quickly identify and recommend verified solutions.
    """)
    
    # Load all analysis results from agentcore_output
    agentcore_file = Path('data/agentcore_output.json')
    
    if not agentcore_file.exists():
        st.warning("No knowledge base data available")
        return
    
    with open(agentcore_file, 'r') as f:
        incidents = json.load(f)
    
    # Knowledge base statistics
    st.markdown("### üìä Knowledge Base Statistics")
    
    knowledge_items = []
    for inc in incidents:
        root_cause = inc.get('root_cause', 'unknown')
        confidence = inc.get('confidence', 0)
        incident_id = inc.get('incident_id', '')
        
        # Check if there are optimization results
        opt_results = load_optimization_results(incident_id)
        success = len(opt_results) > 0
        
        knowledge_items.append({
            'incident_id': incident_id,
            'root_cause': root_cause,
            'confidence': confidence,
            'has_solution': success,
            'actions_count': len(opt_results)
        })
    
    knowledge_df = pd.DataFrame(knowledge_items)
    
    if not knowledge_df.empty:
        # Root cause statistics
        st.markdown("#### üîç Root Cause Distribution")
        root_cause_counts = knowledge_df['root_cause'].value_counts()
        
        fig = px.pie(
            values=root_cause_counts.values,
            names=root_cause_counts.index,
            title="Known Root Cause Type Distribution"
        )
        st.plotly_chart(fig, use_container_width=True)
        
        # Knowledge base table
        st.markdown("#### üìã Experience Knowledge Base")
        st.dataframe(
            knowledge_df[['incident_id', 'root_cause', 'confidence', 'has_solution', 'actions_count']],
            use_container_width=True,
            hide_index=True
        )
        
        # Learning effect
        st.markdown("### üéØ Learning Effect")
        st.success(f"""
        - **Accumulated Experience**: {len(knowledge_df)} incident cases
        - **With Solutions**: {len(knowledge_df[knowledge_df['has_solution']])} cases
        - **Average Confidence**: {knowledge_df['confidence'].mean()*100:.1f}%
        - **Reusability**: Similar incidents can immediately match known solutions, accelerating MTTR
        """)


def main():
    st.set_page_config(
        page_title="Network Autonomic Maintenance Agent",
        page_icon="üåê",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    # Main title
    st.title("üåê AI-Driven Self-Healing Network Operations System")

    
    # Sidebar navigation
    st.sidebar.title("üìë Navigation Menu")
    page = st.sidebar.radio(
        "Select Page",
        [
            "üè† System Overview",
            "üåê Scenario Comparison (RAN + Transport)",
            "üß† Agent Reasoning Layer",
            "üîí Safety Audit",
            "üìö Knowledge Base",
        ]
    )
    
    if page == "üè† System Overview":
        display_autonomous_healing_overview()
        df = load_network_metrics()
        if df is not None:
            display_metrics_overview(df)
    
    elif page == "üåê Scenario Comparison (RAN + Transport)":
        display_scenario_comparison()
    
    elif page == "üß† Agent Reasoning Layer":
        display_agent_reasoning_layer()
    
    elif page == "üîí Safety Audit":
        display_safety_audit()
    
    elif page == "üìö Knowledge Base":
        display_knowledge_base()


if __name__ == '__main__':
    main()
